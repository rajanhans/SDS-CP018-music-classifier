{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlow is used for building the CNN model, while \n",
    "# scikit-learn is used for data preprocessing. \n",
    "# Pandas handles data manipulation \n",
    "# and joblib is used to save model components (scaler and label encoder).\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Reshape, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib  # For saving scaler and label encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and extract features and labels from provided features_3_sec csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv(\"../../Misc/features_3_sec.csv\")\n",
    "X = features_df.drop(columns=['label']).values  # Drop 'label' column and take all features\n",
    "y = features_df['label'].values  # 'label' column contains the target classes (genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML models generally work with numerical data, so we need to encode the music genres (which are categorical) into numerical labels. \n",
    "# The LabelEncoder will map each unique genre to an integer.\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Save the encoder so it can be reused after transforming the labels \n",
    "# later (for example, during model inference).\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"LabelEncoder saved successfully.\")\n",
    "\n",
    "# Normalize the features to ensure that each feature has zero mean and unit variance. This is important for many machine learning algorithms, as \n",
    "# it ensures that no feature dominates the model due to differences in scale.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the fitted scaler  using joblib for future use.\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Scaler saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spilt into Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into a training set (80%) and a testing set (20%) using the train_test_split function. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the shape of the training and testing data to verify the split.\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Reshape the input to have a 3D shape (samples, timesteps, features) for Conv1D layers\n",
    "# We add an extra dimension since Conv1D expects a 3D input.\n",
    "model.add(Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# First Conv1D layer with 32 filters, a kernel size of 3, ReLU activation, and L2 regularization\n",
    "model.add(Conv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "# Batch normalization to help stabilize training by normalizing activations\n",
    "model.add(BatchNormalization())\n",
    "# Max-pooling layer to down-sample the feature map\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "# Second Conv1D layer with 64 filters, kernel size of 3, ReLU activation, and L2 regularization\n",
    "model.add(Conv1D(64, 3, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "# Another max-pooling layer\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "# Flatten the feature map to create a 1D vector, which is required for the dense layers\n",
    "model.add(Flatten())\n",
    "# Dense layer with 64 units, ReLU activation, and L2 regularization\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "# Dropout layer to prevent overfitting by randomly dropping 30% of the neurons\n",
    "model.add(Dropout(0.3))\n",
    "# Output layer with softmax activation for multi-class classification\n",
    "model.add(Dense(len(np.unique(y_encoded)), activation='softmax'))\n",
    "\n",
    "# Compile the model using Adam optimizer with a learning rate of 0.001 and sparse categorical cross-entropy loss\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define early stopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping is a callback function that helps prevent overfitting by stopping \n",
    "# the training process if the validation loss doesn't improve after a certain \n",
    "# number of epochs (patience=5). This will restore the best weights based on \n",
    "# the validation performance.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model using the training data.\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# save the model to disk using the .save() method. \n",
    "model.save('music_genre_cnn_model_regularized.h5')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the trained model on the test set to determine its performance \n",
    "# in terms of accuracy. This gives us an indication of how well the model \n",
    "# generalizes to unseen data.\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training history for accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation accuracy and loss over epochs to visualize \n",
    "# how the model improved during training. This can help identify potential \n",
    "# issues like overfitting or underfitting.\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
