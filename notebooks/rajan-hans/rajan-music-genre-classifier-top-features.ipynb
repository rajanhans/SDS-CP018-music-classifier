{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Read full features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the features CSV file\n",
    "file_path = \"../../Misc/features_3_sec.csv\"\n",
    "full_features_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(full_features_df.head())         # Display the first few rows\n",
    "print(full_features_df.info())         # Check for null values and data types\n",
    "print(full_features_df.describe())     # Summary statistics for numerical columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if full_features_df.isnull().sum().sum() > 0:\n",
    "    print(full_features_df.isnull().sum())\n",
    "    #Option 1: Remove rows with missing values\n",
    "    data = full_features_df.dropna()\n",
    "\n",
    "    #Option 2: Impute missing values (e.g., with the column mean)\n",
    "    #data = full_features_df.fillna(full_features_df.mean())\n",
    "    #print(full_features_df.isnull().sum())\n",
    "else:\n",
    "    print(\"No missing values found in the dataset\")\n",
    "\n",
    "# Verify there are no missing values left\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 -  Train an XGBoost classifier to identify top 'n' number important features in Full features file. \n",
    "No need to split the dataset at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Encode the categorical label (genre) using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "full_features_df['label_encoded'] = label_encoder.fit_transform(full_features_df['label'])\n",
    "\n",
    "# Drop irrelevant columns\n",
    "# Remove columns that don't contribute to the prediction\n",
    "processed_df = full_features_df.drop(columns=['filename', 'label'])\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = processed_df.drop(columns=['label_encoded'])\n",
    "y = processed_df['label_encoded']\n",
    "\n",
    "\n",
    "\n",
    "# Train the model - no \n",
    "model = XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Sort Feature based on importance\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "sorted_feature_names = [feature_names[i] for i in indices]\n",
    "sorted_importances = importances[indices]\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(sorted_feature_names, sorted_importances, color='blue')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance Analysis (Sorted)\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display important features at the top\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Extract top 'n' features and drop the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top 'n' features - in this case it is 15\n",
    "top_n = 15\n",
    "top_features = [feature_names[i] for i in indices[:top_n]]\n",
    "\n",
    "print(\"Top Features:\")\n",
    "for i, feature in enumerate(top_features, 1):\n",
    "    print(f\"{i}. {feature}: {importances[indices[i-1]]:.4f}\")\n",
    "\n",
    "# Drop non-important features\n",
    "X_reduced = X[top_features]  # Select only the top features\n",
    "\n",
    "# Combine the reduced features with the target (label) column safely\n",
    "X_reduced_with_label = pd.concat([X_reduced, y], axis=1)\n",
    "\n",
    "# Save the reduced features file\n",
    "reduced_file_path = \"reduced_features.csv\"\n",
    "X_reduced_with_label.to_csv(reduced_file_path, index=False)\n",
    "\n",
    "print(f\"Reduced features CSV saved at: {reduced_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 -  Encode categorical label (genre) on the reduced features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "#Read reduced feature file saved above\n",
    "reduced_features_df = pd.read_csv(reduced_file_path)\n",
    "\n",
    "# Split the reduced features into features and target\n",
    "X = reduced_features_df.drop(columns=[\"label_encoded\"])\n",
    "y = reduced_features_df[\"label_encoded\"]\n",
    "\n",
    "# Encode the categorical label (genre) using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the scaler and label encoder\n",
    "dump(scaler, 'scaler.joblib')\n",
    "dump(label_encoder, 'label_encoder.joblib')\n",
    "\n",
    "print(f\"Shape of features (X): {X.shape}\")\n",
    "print(f\"Shape of target (y): {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 - Split the reduced features dataset into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split( X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Print dataset shapes and verify\n",
    "print(f\"Training Features Shape: {X_train.shape}\")\n",
    "print(f\"Testing Features Shape: {X_test.shape}\")\n",
    "print(f\"Training Labels Shape: {y_train.shape}\")\n",
    "print(f\"Testing Labels Shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7 - Run XGBoost Classifier on Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Classifier\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb) * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "#print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8 - Save the XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save the model to a file\n",
    "dump(xgb_model, 'genre_class_model_xgboost.joblib')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Step - Test using a sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the pre-trained XGBoost model, scaler, and label encoder\n",
    "xgb_model = load('genre_class_model_xgboost.joblib')\n",
    "scaler = load('scaler.joblib')\n",
    "label_encoder = load('label_encoder.joblib')\n",
    "\n",
    "# Define the genre names corresponding to the encoded labels\n",
    "genre_names = ['blues','classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']  \n",
    "\n",
    "# Step 2: Function to extract specified features from a .wav file\n",
    "def extract_features_from_wav(wav_file):\n",
    "    # Load audio file using librosa\n",
    "    y, sr = librosa.load(wav_file, sr=None)  # sr=None preserves the original sample rate\n",
    "    \n",
    "    # Extract various audio features per reduced feature set \n",
    "    \n",
    "    # Spectral Bandwidth\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spectral_bandwidth_mean = np.mean(spectral_bandwidth)\n",
    "    \n",
    "    # Chroma STFT (Short-Time Fourier Transform)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_stft_mean = np.mean(chroma_stft)\n",
    "    chroma_stft_var = np.var(chroma_stft)\n",
    "    \n",
    "    # MFCCs (Mel-frequency cepstral coefficients)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=6)\n",
    "    mfcc1_mean = np.mean(mfcc[0])\n",
    "    mfcc1_var = np.var(mfcc[0])\n",
    "    mfcc4_mean = np.mean(mfcc[3])\n",
    "    mfcc5_var = np.var(mfcc[4])\n",
    "    mfcc6_mean = np.mean(mfcc[5])\n",
    "    \n",
    "    # Roll-off (Spectral roll-off point)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=0.85)\n",
    "    rolloff_mean = np.mean(rolloff)\n",
    "    rolloff_var = np.var(rolloff)\n",
    "    \n",
    "    # Root Mean Square (RMS)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    rms_var = np.var(rms)\n",
    "    \n",
    "    # Spectral Centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_centroid_var = np.var(spectral_centroid)\n",
    "    \n",
    "    # Tempo (beats per minute)\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    \n",
    "    # Harmony (harmonic-to-noise ratio)\n",
    "    harmony = librosa.effects.harmonic(y)\n",
    "    harmony_mean = np.mean(harmony)\n",
    "    \n",
    "    # A simple estimate of perceptual variance could be the variance in the RMS or spectral centroid\n",
    "    perceptr_var = np.var(rms)  # Using RMS variance as a proxy for perceptual variance\n",
    "    \n",
    "    # Combine all extracted features\n",
    "    features = [\n",
    "        perceptr_var, spectral_bandwidth_mean, chroma_stft_mean, mfcc4_mean, chroma_stft_var, \n",
    "        mfcc1_var, rolloff_var, rms_var, rolloff_mean, mfcc1_mean, \n",
    "        spectral_centroid_var, mfcc5_var, mfcc6_mean, tempo, harmony_mean\n",
    "    ]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Step 3: Extract features from a sample input .wav file\n",
    "wav_file = 'khisco.00008.wav'  # Replace with your file path\n",
    "extracted_features = extract_features_from_wav(wav_file)\n",
    "\n",
    "# Step 4: Convert the features into a DataFrame with the correct column names\n",
    "reduced_feature_columns = [\n",
    "    \"perceptr_var\", \"spectral_bandwidth_mean\", \"chroma_stft_mean\", \"mfcc4_mean\", \n",
    "    \"chroma_stft_var\", \"mfcc1_var\", \"rolloff_var\", \"rms_var\", \"rolloff_mean\", \n",
    "    \"mfcc1_mean\", \"spectral_centroid_var\", \"mfcc5_var\", \"mfcc6_mean\", \"tempo\", \n",
    "    \"harmony_mean\"\n",
    "]\n",
    "\n",
    "# Ensure the features list has the correct number of features\n",
    "extracted_features_df = pd.DataFrame([extracted_features], columns=reduced_feature_columns)\n",
    "\n",
    "# Step 5: Scale the extracted features using the saved scaler\n",
    "X_scaled = scaler.transform(extracted_features_df)\n",
    "\n",
    "# Step 6: Use the trained model to predict the genre\n",
    "predicted_label_encoded = xgb_model.predict(X_scaled)\n",
    "\n",
    "# Step 7: Decode the predicted label to get the genre name\n",
    "predicted_label = label_encoder.inverse_transform(predicted_label_encoded)\n",
    "\n",
    "# Step 8: Map the encoded label to the actual genre name\n",
    "predicted_genre_name = genre_names[predicted_label[0]]  # Map the predicted label to the actual genre name\n",
    "\n",
    "# Step 9: Print the predicted genre\n",
    "print(f\"The predicted genre of the song is: {predicted_genre_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
