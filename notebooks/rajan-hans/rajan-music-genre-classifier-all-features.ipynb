{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Read full features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the features CSV file\n",
    "file_path = \"../../Misc/features_3_sec.csv\"\n",
    "full_features_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(full_features_df.head())         # Display the first few rows\n",
    "print(full_features_df.info())         # Check for null values and data types\n",
    "print(full_features_df.describe())     # Summary statistics for numerical columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Encode categorical label (genre) on the features file and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features (X): (9990, 58)\n",
      "Shape of target (y): (9990,)\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "full_features_df['label'] = label_encoder.fit_transform(full_features_df['label'])\n",
    "\n",
    "# Encode the categorical label (genre) using LabelEncoder\n",
    "X = full_features_df.drop(columns=['filename', 'label', 'length'])\n",
    "y = full_features_df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# lets scale the data \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler and label encoder\n",
    "dump(scaler, 'scaler.joblib')\n",
    "dump(label_encoder, 'label_encoder.joblib')\n",
    "\n",
    "print(f\"Shape of features (X): {X.shape}\")\n",
    "print(f\"Shape of target (y): {y.shape}\")\n",
    "# Print dataset shapes and verify\n",
    "print(f\"Training Features Shape: {X_train.shape}\")\n",
    "print(f\"Testing Features Shape: {X_test.shape}\")\n",
    "print(f\"Training Labels Shape: {y_train.shape}\")\n",
    "print(f\"Testing Labels Shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7 - Run XGBoost Classifier on Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=29)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Classifier\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb) * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "#print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n",
    "\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"True\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate code for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Initialize the XGBoost model with class imbalance handling\n",
    "xgb_model = XGBClassifier(\n",
    "    eval_metric='mlogloss', \n",
    "    random_state=29, \n",
    "    scale_pos_weight=1,  # Set scale_pos_weight if there's class imbalance (adjust accordingly)\n",
    "    max_depth=5,         # Tuning hyperparameters for better recall\n",
    "    learning_rate=0.1,   # Tuning learning rate\n",
    "    n_estimators=1000    # Number of boosting rounds (trees)\n",
    ")\n",
    "\n",
    "# Step 2: Train the model with training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Predict on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the model using recall\n",
    "print(\"XGBoost Classifier\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb) * 100:.2f}%\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_xgb, average='macro') * 100:.2f}%\")  # Print recall\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Adjust the classification threshold to increase recall\n",
    "# We will try adjusting the threshold for the positive class to get a better recall.\n",
    "threshold = 0.3  # Example threshold, you can experiment with different values\n",
    "y_pred_probs = xgb_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "y_pred_adjusted = (y_pred_probs > threshold).astype(int)  # Adjusted prediction based on threshold\n",
    "\n",
    "# Step 6: Evaluate adjusted prediction\n",
    "print(\"\\nAdjusted Classification Report (Threshold = 0.3):\")\n",
    "print(classification_report(y_test, y_pred_adjusted))\n",
    "\n",
    "# Plot confusion matrix for adjusted threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_adjusted), annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Adjusted Confusion Matrix (Threshold = 0.3)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8 - Save the XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genre_class_model_xgboost_full.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save the model to a file\n",
    "dump(xgb_model, 'genre_class_model_xgboost_full.joblib')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Step - Test using a sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- length\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 119\u001b[0m\n\u001b[0;32m    116\u001b[0m extracted_features_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([extracted_features], columns\u001b[38;5;241m=\u001b[39mfeature_columns)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Step 8: Scale the extracted features using the saved scaler\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_features_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Step 9: Use the trained model to predict the genre\u001b[39;00m\n\u001b[0;32m    122\u001b[0m predicted_label_encoded \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_scaled)\n",
      "File \u001b[1;32mc:\\MyWork\\Tech-Work\\SDS\\SDS-CP018-music-classifier\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\MyWork\\Tech-Work\\SDS\\SDS-CP018-music-classifier\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\MyWork\\Tech-Work\\SDS\\SDS-CP018-music-classifier\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2919\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[0;32m   2836\u001b[0m     _estimator,\n\u001b[0;32m   2837\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m   2844\u001b[0m ):\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \n\u001b[0;32m   2847\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2919\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2920\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[0;32m   2921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[1;32mc:\\MyWork\\Tech-Work\\SDS\\SDS-CP018-music-classifier\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2777\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m   2775\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2777\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- length\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the pre-trained XGBoost model, scaler, and label encoder\n",
    "xgb_model = load('genre_class_model_xgboost_full.joblib')\n",
    "scaler = load('scaler.joblib')\n",
    "label_encoder = load('label_encoder.joblib')\n",
    "\n",
    "# Define the genre names corresponding to the encoded labels\n",
    "genre_names = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "# Step 2: Function to extract all features from a .wav file\n",
    "def extract_all_features_from_wav(wav_file):\n",
    "    # Load audio file using librosa\n",
    "    y, sr = librosa.load(wav_file, sr=None)  # sr=None preserves the original sample rate\n",
    "    \n",
    "    # Extract various audio features (matching CSV columns)\n",
    "    \n",
    "    # Length of the audio (duration in seconds)\n",
    "    #length = librosa.get_duration(y=y, sr=sr)\n",
    "    \n",
    "    # Spectral Bandwidth\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spectral_bandwidth_mean = np.mean(spectral_bandwidth)\n",
    "    spectral_bandwidth_var = np.var(spectral_bandwidth)\n",
    "    \n",
    "    # Chroma STFT (Short-Time Fourier Transform)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_stft_mean = np.mean(chroma_stft)\n",
    "    chroma_stft_var = np.var(chroma_stft)\n",
    "    \n",
    "    # RMS (Root Mean Square)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_var = np.var(rms)\n",
    "    \n",
    "    # Spectral Centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "    spectral_centroid_var = np.var(spectral_centroid)\n",
    "    \n",
    "    # Roll-off (Spectral roll-off point)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=0.85)\n",
    "    rolloff_mean = np.mean(rolloff)\n",
    "    rolloff_var = np.var(rolloff)\n",
    "    \n",
    "    # Zero-Crossing Rate\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y)\n",
    "    zero_crossing_rate_mean = np.mean(zero_crossing_rate)\n",
    "    zero_crossing_rate_var = np.var(zero_crossing_rate)\n",
    "    \n",
    "    # Harmony (harmonic-to-noise ratio)\n",
    "    harmony = librosa.effects.harmonic(y)\n",
    "    harmony_mean = np.mean(harmony)\n",
    "    harmony_var = np.var(harmony)\n",
    "    \n",
    "    # Perceptual features\n",
    "    perceptr_mean = np.mean(rms)  # Simplified perceptual mean\n",
    "    perceptr_var = np.var(rms)    # Simplified perceptual variance\n",
    "    \n",
    "    # Tempo (beats per minute)\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    \n",
    "    # MFCCs (Mel-frequency cepstral coefficients)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    mfcc_features = [np.mean(mfcc[i]) for i in range(20)] + [np.var(mfcc[i]) for i in range(20)]\n",
    "    \n",
    "    # Combine all extracted features\n",
    "    features = [\n",
    "        chroma_stft_mean, chroma_stft_var, rms_mean, rms_var,\n",
    "        spectral_centroid_mean, spectral_centroid_var, spectral_bandwidth_mean, spectral_bandwidth_var,\n",
    "        rolloff_mean, rolloff_var, zero_crossing_rate_mean, zero_crossing_rate_var,\n",
    "        harmony_mean, harmony_var, perceptr_mean, perceptr_var, tempo\n",
    "    ] + mfcc_features\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Step 3: Extract features from a sample input .wav file\n",
    "#wav_file = 'klass.00010.wav'  \n",
    "wav_file ='Staylin_alive.wav'\n",
    "extracted_features = extract_all_features_from_wav(wav_file)\n",
    "# un-comment below section for debugging purpose\n",
    "# print(f\"Extracted features (length): {len(extracted_features)}\")\n",
    "# print(f\"Extracted features: {extracted_features}\")\n",
    "\n",
    "# Step 4: Get all feature column names from the CSV file\n",
    "feature_columns = pd.read_csv(\"../../Misc/features_3_sec.csv\").columns.tolist()\n",
    "\n",
    "# un-comment below section for debugging purpose\n",
    "#print(f\"Feature columns from CSV (length): {len(feature_columns)}\")\n",
    "\n",
    "# Step 5: Clean the feature columns (remove 'label' and 'filename')\n",
    "feature_columns.remove('label')\n",
    "feature_columns.remove('filename')\n",
    "feature_columns.remove('length')\n",
    "\n",
    "# un-comment below section for debugging purpose\n",
    "#print(f\"Feature columns after cleaning (length): {len(feature_columns)}\")\n",
    "\n",
    "# un-comment below section for debugging purpose\n",
    "# # Step 6: Compare the number of extracted features to the expected number of features\n",
    "# if len(extracted_features) != len(feature_columns):\n",
    "#     print(f\"Warning: Number of extracted features ({len(extracted_features)}) does not match the expected number of features ({len(feature_columns)})\")\n",
    "#     print(f\"Extracted Features: {extracted_features}\")\n",
    "#     print(f\"Feature Columns from CSV: {feature_columns}\")\n",
    "# else:\n",
    "#     print(\"Number of extracted features matches the expected columns.\")\n",
    "\n",
    "# un-comment for debugging purpose\n",
    "#for i in range(len(feature_columns)):\n",
    "#    print(f\"{feature_columns[i]}: {extracted_features[i]}\")\n",
    "\n",
    "# Step 7: Create a DataFrame with the correct columns\n",
    "extracted_features_df = pd.DataFrame([extracted_features], columns=feature_columns)\n",
    "\n",
    "# Step 8: Scale the extracted features using the saved scaler\n",
    "X_scaled = scaler.transform(extracted_features_df)\n",
    "\n",
    "# Step 9: Use the trained model to predict the genre\n",
    "predicted_label_encoded = xgb_model.predict(X_scaled)\n",
    "\n",
    "# Step 10: Decode the predicted label to get the genre name\n",
    "predicted_label = label_encoder.inverse_transform(predicted_label_encoded)\n",
    "\n",
    "# Step 11: Map the encoded label to the actual genre name\n",
    "predicted_genre_name = genre_names[predicted_label[0]]  # Map the predicted label to the actual genre name\n",
    "\n",
    "# Step 12: Print the predicted genre\n",
    "print(f\"The predicted genre of the song {wav_file} is: {predicted_genre_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
